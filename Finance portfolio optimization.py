# -*- coding: utf-8 -*-
""" Finance portfolio optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4Io9Gg-h5Pbf4XZNkcnH7yrH95nei6i

# Importing Libraries & Mounting Dataset
"""

# Essential Libraries
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc

# Suppress warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Memory optimization settings
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

# Memory usage tracker
def check_memory_usage():
    import psutil
    process = psutil.Process()
    memory_usage = process.memory_info().rss / 1024 / 1024  # in MB
    return f"Current Memory Usage: {memory_usage:.2f} MB"

# Execution timer
from contextlib import contextmanager
import time

@contextmanager
def timer(task_name):
    start_time = time.time()
    yield
    elapsed_time = time.time() - start_time
    print(f"{task_name} completed in {elapsed_time:.2f} seconds")

# Project settings for Indian Market
CONFIG = {
    'start_date': '2020-01-01',
    'end_date': '2025-02-09',
    # Major Indian stocks from different sectors
    'symbols': ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS',
                'ICICIBANK.NS', 'HINDUNILVR.NS', 'ITC.NS', 'SBIN.NS',
                'BHARTIARTL.NS', 'KOTAKBANK.NS'],
    'market_index': '^NSEI',  # NIFTY 50 index
    'lookback_period': 252,  # Trading days in a year
    'batch_size': 1000,
    'train_test_split': 0.8,
    'random_state': 42
}

# Create project directories
def setup_project_structure():
    base_path = '/content/drive/MyDrive/indian_portfolio_optimization'
    directories = ['data', 'models', 'results', 'logs']

    for directory in directories:
        path = f"{base_path}/{directory}"
        !mkdir -p $path
    return base_path

"""# DATA COLLECTION"""

class IndianMarketData:
    def __init__(self, config):
        self.config = config

    def fetch_market_data(self):
        """Fetch data for Indian market symbols"""
        data = yf.download(
            self.config['symbols'] + [self.config['market_index']],
            start=self.config['start_date'],
            end=self.config['end_date'],
            interval='1d'
        )
        return data['Adj Close']

    def calculate_market_indicators(self, data):
        """Calculate Indian market specific indicators"""
        # VIX equivalent for Indian market
        india_vix = yf.download('^INDIAVIX',
                              start=self.config['start_date'],
                              end=self.config['end_date'])['Adj Close']
        return india_vix

def monitor_indian_market_performance(portfolio, benchmark='^NSEI'):
    """Monitor portfolio performance against NIFTY 50"""
    nifty50 = yf.download(benchmark,
                         start=CONFIG['start_date'],
                         end=CONFIG['end_date'])['Adj Close']

    performance_metrics = {
        'Portfolio_Return': portfolio.pct_change().mean() * 252,
        'Portfolio_Volatility': portfolio.pct_change().std() * np.sqrt(252),
        'Nifty50_Return': nifty50.pct_change().mean() * 252,
        'Nifty50_Volatility': nifty50.pct_change().std() * np.sqrt(252)
    }
    return performance_metrics

# Essential Libraries
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc
import logging

# Suppress warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Performance Monitoring
@contextmanager
def timer(task_name):
    start_time = time.time()
    yield
    elapsed_time = time.time() - start_time
    print(f"{task_name} completed in {elapsed_time:.2f} seconds")

# Setup logging function
def setup_logging(base_path):
    logging.basicConfig(
        filename=f"{base_path}/logs/portfolio_optimization.log",
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def setup_project_structure():
    base_path = '/content/drive/MyDrive/indian_portfolio_optimization'
    directories = ['data', 'models', 'results', 'logs']

    for directory in directories:
        path = f"{base_path}/{directory}"
        !mkdir -p $path
    return base_path

def initialize_environment():
    with timer("Environment Setup"):
        base_path = setup_project_structure()
        logger = setup_logging(base_path)
        market_data = IndianMarketData(CONFIG)

        logger.info("Indian Market Environment setup completed")
        print(check_memory_usage())
    return base_path, market_data

def fetch_indian_market_data():
    with timer("Data Collection"):
        # Fetch stock data
        stock_data = yf.download(
            CONFIG['symbols'],
            start=CONFIG['start_date'],
            end=CONFIG['end_date']
        )['Adj Close']

        # Fetch NIFTY data for benchmark
        nifty_data = yf.download(
            CONFIG['market_index'],
            start=CONFIG['start_date'],
            end=CONFIG['end_date']
        )['Adj Close']

        return stock_data, nifty_data

def preprocess_data(stock_data):
    # Calculate returns
    returns = stock_data.pct_change()

    # Remove missing values
    returns = returns.dropna()

    # Calculate basic features
    rolling_mean = returns.rolling(window=20).mean()
    rolling_std = returns.rolling(window=20).std()

    return returns, rolling_mean, rolling_std

def collect_market_data():
    try:
        # Fetch stock and index data
        stock_data, nifty_data = fetch_indian_market_data()

        print(f"Data collected from {CONFIG['start_date']} to {CONFIG['end_date']}")

        # Basic data quality checks
        if stock_data.isnull().sum().sum() > 0:
            print(f"Missing values detected: {stock_data.isnull().sum().sum()}")
            stock_data = stock_data.fillna(method='ffill')

        return stock_data, nifty_data
    except Exception as e:
        logging.error(f"Error in data collection: {str(e)}")
        raise

"""# DATA PREPROCESSING"""

def verify_data_quality(stock_data, nifty_data):
    quality_report = {
        'total_rows': len(stock_data),
        'date_range': f"{stock_data.index.min()} to {stock_data.index.max()}",
        'missing_values': stock_data.isnull().sum(),
        'stocks_with_data': stock_data.columns.tolist(),
        'nifty_data_available': not nifty_data.empty
    }

    # Check for extreme values
    for column in stock_data.columns:
        zscore = np.abs((stock_data[column] - stock_data[column].mean()) / stock_data[column].std())
        outliers = np.where(zscore > 3)[0]
        if len(outliers) > 0:
            print(f"Outliers detected in {column}: {len(outliers)} points")

    return quality_report

def collect_market_data():
    try:
        # Fetch stock data with specific columns
        stock_data = yf.download(
            CONFIG['symbols'],
            start=CONFIG['start_date'],
            end=CONFIG['end_date'],
            progress=True
        )

        # Handle the new data structure
        if 'Close' in stock_data.columns:
            # Create Adj Close from Close if not present
            if 'Adj Close' not in stock_data.columns:
                stock_data['Adj Close'] = stock_data['Close']

                # Adjust for splits and dividends
                for symbol in CONFIG['symbols']:
                    ticker = yf.Ticker(symbol)
                    splits = ticker.splits
                    dividends = ticker.dividends

                    # Apply adjustments if any exist
                    if not splits.empty or not dividends.empty:
                        stock_data['Adj Close'][symbol] = stock_data['Close'][symbol] * (1 + dividends/stock_data['Close'][symbol])

        return stock_data
    except Exception as e:
        logging.error(f"Error in data collection: {str(e)}")
        raise

def save_to_cache(data, cache_name):
    try:
        # Ensure data is in the correct format before saving
        if isinstance(data, pd.DataFrame):
            cache_file = f"{base_path}/data/cache/{cache_name}.csv"
            data.to_csv(cache_file)
            print(f"Data saved to cache: {cache_name}")
    except Exception as e:
        logging.error(f"Cache save error: {str(e)}")

"""# FEATURE ENGINEERING"""

def generate_technical_features(data):
    features = pd.DataFrame(index=data.index)

    for symbol in data.columns:
        # Price-based indicators
        features[f'{symbol}_returns'] = data[symbol].pct_change()

        # Volatility (21-day rolling - Indian market monthly equivalent)
        features[f'{symbol}_volatility'] = (
            features[f'{symbol}_returns']
            .rolling(window=21)
            .std() * np.sqrt(252)
        )

        # Moving averages
        features[f'{symbol}_SMA20'] = data[symbol].rolling(window=20).mean()
        features[f'{symbol}_SMA50'] = data[symbol].rolling(window=50).mean()
        features[f'{symbol}_SMA200'] = data[symbol].rolling(window=200).mean()

def generate_market_features(data):
    market_features = pd.DataFrame(index=data.index)

    # Market momentum
    nifty_returns = data['^NSEI'].pct_change()
    market_features['market_momentum'] = nifty_returns.rolling(window=10).mean()

    # Market volatility
    market_features['market_volatility'] = (
        nifty_returns.rolling(window=21).std() * np.sqrt(252)
    )

    # Relative strength
    for symbol in data.columns:
        if symbol != '^NSEI':
            market_features[f'{symbol}_rel_strength'] = (
                data[symbol].pct_change() - nifty_returns
            )

    return market_features

def add_economic_features(features):
    # Current economic parameters based on recent data
    features['interest_rate'] = 6.25  # Current RBI rate[5]
    features['gdp_growth'] = 6.4  # Current GDP growth rate[28]
    features['inflation'] = 5.22  # Current inflation rate[38]

    return features

def create_feature_matrix(stock_data, market_data):
    features = pd.DataFrame(index=stock_data.index)

    # Generate all features
    technical_features = generate_technical_features(stock_data)
    market_features = generate_market_features(market_data)

    # Combine features
    features = pd.concat([
        technical_features,
        market_features
    ], axis=1)

    # Add economic indicators
    features = add_economic_features(features)

    # Remove missing values
    features = features.dropna()

    return features

def select_features(features, threshold=0.95):
    # Remove highly correlated features
    correlation_matrix = features.corr().abs()
    upper = correlation_matrix.where(
        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
    )
    to_drop = [column for column in upper.columns
               if any(upper[column] > threshold)]

    return features.drop(columns=to_drop)

# Import required libraries
import numpy as np
import pandas as pd
import yfinance as yf
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Configuration
CONFIG = {
    'start_date': '2020-01-01',
    'end_date': '2024-02-09',
    'symbols': ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS',
                'ICICIBANK.NS', 'HINDUNILVR.NS', 'ITC.NS', 'SBIN.NS',
                'BHARTIARTL.NS', 'KOTAKBANK.NS'],
    'market_index': '^NSEI'  # NIFTY 50 index
}

# Fetch Data
def fetch_stock_data():
    print("Fetching stock data...")
    stock_data = yf.download(
        CONFIG['symbols'],
        start=CONFIG['start_date'],
        end=CONFIG['end_date']
    )['Close']  # Using 'Close' instead of 'Adj Close'

    print("Fetching Nifty data...")
    nifty_data = yf.download(
        CONFIG['market_index'],
        start=CONFIG['start_date'],
        end=CONFIG['end_date']
    )['Close']

    return stock_data, nifty_data

# Data Overview
def show_data_overview(stock_data, nifty_data):
    print("\nStock Data Sample:")
    print(stock_data.head())
    print("\nStock Data Info:")
    print(stock_data.info())

    print("\nNifty Data Sample:")
    print(nifty_data.head())

# Technical Analysis
def calculate_technical_indicators(stock_data):
    tech_indicators = pd.DataFrame(index=stock_data.index)

    for symbol in stock_data.columns:
        # Returns
        tech_indicators[f'{symbol}_returns'] = stock_data[symbol].pct_change()

        # Volatility
        tech_indicators[f'{symbol}_volatility'] = (
            tech_indicators[f'{symbol}_returns']
            .rolling(window=21)
            .std() * np.sqrt(252)
        )

        # Moving averages
        tech_indicators[f'{symbol}_SMA20'] = stock_data[symbol].rolling(window=20).mean()
        tech_indicators[f'{symbol}_SMA50'] = stock_data[symbol].rolling(window=50).mean()

    return tech_indicators

# Visualization
def plot_stock_analysis(stock_data, tech_indicators, symbol='RELIANCE.NS'):
    plt.figure(figsize=(15, 10))

    # Price and Moving Averages
    plt.subplot(2, 1, 1)
    plt.plot(stock_data[symbol], label='Price')
    plt.plot(tech_indicators[f'{symbol}_SMA20'], label='20-day SMA')
    plt.plot(tech_indicators[f'{symbol}_SMA50'], label='50-day SMA')
    plt.title(f'{symbol} Price and Moving Averages')
    plt.legend()

    # Returns and Volatility
    plt.subplot(2, 1, 2)
    plt.plot(tech_indicators[f'{symbol}_volatility'], label='Volatility')
    plt.title(f'{symbol} Volatility')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Main execution
if __name__ == "__main__":
    # Fetch data
    stock_data, nifty_data = fetch_stock_data()

    # Show data overview
    show_data_overview(stock_data, nifty_data)

    # Calculate technical indicators
    tech_indicators = calculate_technical_indicators(stock_data)

    # Plot analysis for RELIANCE
    plot_stock_analysis(stock_data, tech_indicators, 'RELIANCE.NS')

    # Show correlation matrix
    plt.figure(figsize=(12, 8))
    sns.heatmap(stock_data.corr(), cmap='RdYlBu', center=0, annot=True)
    plt.title('Stock Price Correlation Matrix')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

# First, ensure we have the data
def visualize_features():
    # 1. Calculate technical indicators
    tech_indicators = calculate_technical_indicators(stock_data)

    # 2. Display sample of features
    print("\nTechnical Indicators Sample:")
    print(tech_indicators.head())

    # 3. Visualize features
    plt.figure(figsize=(15, 10))

    # Plot different features
    plt.subplot(2, 2, 1)
    tech_indicators['RELIANCE.NS_volatility'].plot()
    plt.title('Reliance Volatility')

    plt.subplot(2, 2, 2)
    tech_indicators['RELIANCE.NS_SMA20'].plot()
    plt.title('Reliance 20-day SMA')

    plt.subplot(2, 2, 3)
    tech_indicators['RELIANCE.NS_returns'].hist(bins=50)
    plt.title('Returns Distribution')

    plt.subplot(2, 2, 4)
    sns.heatmap(tech_indicators.corr(), cmap='RdYlBu')
    plt.title('Feature Correlation')

    plt.tight_layout()
    plt.show()

# Execute visualization
if __name__ == "__main__":
    try:
        visualize_features()
    except Exception as e:
        print(f"Error in visualization: {str(e)}")

"""# MODEL TRAINING"""

def create_portfolio_optimizer():
    # Initialize model with LightGBM (more memory efficient for Colab)
    model = LGBMRegressor(
        n_estimators=100,
        learning_rate=0.05,
        max_depth=5,
        random_state=42
    )
    return model

def prepare_training_data(features, returns, lookback=252):
    # Prepare features and targets
    X = features.shift(1).dropna()  # Previous day's features
    y = returns.dropna()  # Next day's returns

    # Ensure alignment
    common_index = X.index.intersection(y.index)
    X = X.loc[common_index]
    y = y.loc[common_index]

    return X, y

def train_model(X, y):
    # Split data into training and validation sets
    train_size = int(len(X) * 0.8)
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:]
    y_val = y[train_size:]

    # Train model
    model = create_portfolio_optimizer()
    model.fit(X_train, y_train)

    return model, X_val, y_val

def generate_portfolio_weights(model_predictions, risk_aversion=0.5):
    # Convert predictions to weights using softmax
    weights = np.exp(model_predictions * risk_aversion)
    weights = weights / weights.sum()

    # Apply constraints
    weights = np.clip(weights, 0, 0.3)  # Max 30% in single stock
    weights = weights / weights.sum()  # Renormalize

    return weights

def evaluate_model(model, X_val, y_val):
    # Make predictions
    predictions = model.predict(X_val)

    # Calculate metrics
    mse = mean_squared_error(y_val, predictions)
    r2 = r2_score(y_val, predictions)

    # Generate weights
    weights = generate_portfolio_weights(predictions)

    return {
        'mse': mse,
        'r2': r2,
        'weights': weights
    }

def prepare_training_data(features, stock_data):
    # Calculate returns for target variable
    returns = stock_data['RELIANCE.NS'].pct_change()  # Using Reliance as target

    # Prepare features and target
    X = features.shift(1).dropna()  # Previous day's features
    y = returns[X.index]  # Align target with features

    return X, y

def generate_portfolio_returns():
    try:
        # Fetch and prepare data
        stock_data, nifty_data = fetch_stock_data()
        tech_indicators = calculate_technical_indicators(stock_data)

        # Prepare training data
        X, y = prepare_training_data(tech_indicators, stock_data)

        # Split data
        train_size = int(len(X) * 0.8)
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:]
        y_val = y[train_size:]

        # Train model
        model = LGBMRegressor(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=5,
            random_state=42
        )
        model.fit(X_train, y_train)

        # Generate predictions and weights
        predictions = model.predict(X_val)
        weights = pd.Series(
            generate_portfolio_weights(predictions),
            index=stock_data.columns
        )

        # Calculate portfolio returns
        portfolio_returns = pd.Series(
            (stock_data.pct_change()[train_size:] * weights).sum(axis=1),
            index=stock_data.index[train_size:]
        )

        return portfolio_returns, weights

    except Exception as e:
        print(f"Error in portfolio generation: {str(e)}")
        return None, None

def generate_portfolio_weights(predictions, n_stocks):
    # Ensure weights sum to 1 and are non-negative
    weights = np.exp(predictions)
    weights = weights / np.sum(weights)
    # Reshape to match number of stocks
    weights = weights[:n_stocks]
    return weights

def prepare_training_data(features, stock_data):
    # Calculate returns for target variable
    target_returns = stock_data['RELIANCE.NS'].pct_change()  # Using Reliance as target

    # Prepare features and target
    X = features.shift(1).dropna()  # Previous day's features
    y = target_returns[X.index]  # Align target with features

    return X, y

def generate_portfolio_returns():
    try:
        # Fetch and prepare data
        stock_data, _ = fetch_stock_data()
        tech_indicators = calculate_technical_indicators(stock_data)

        # Prepare training data
        X, y = prepare_training_data(tech_indicators, stock_data)

        # Split data
        train_size = int(len(X) * 0.8)
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:]

        # Train model with verbose=-1 to suppress warnings
        model = LGBMRegressor(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=5,
            random_state=42,
            verbose=-1
        )
        model.fit(X_train, y_train)

        # Generate predictions and weights
        predictions = model.predict(X_val)
        weights = generate_portfolio_weights(predictions, len(stock_data.columns))

        # Calculate portfolio returns
        portfolio_returns = pd.Series(
            (stock_data.pct_change()[train_size:] * weights).sum(axis=1),
            index=stock_data.index[train_size:]
        )

        return portfolio_returns, pd.Series(weights, index=stock_data.columns)

    except Exception as e:
        print(f"Error in portfolio generation: {str(e)}")
        return None, None

def visualize_model_performance():
    try:
        # Fetch and prepare data
        stock_data, nifty_data = fetch_stock_data()
        tech_indicators = calculate_technical_indicators(stock_data)

        # Create subplots
        plt.figure(figsize=(20, 12))

        # 1. Stock Price Performance
        plt.subplot(2, 2, 1)
        for symbol in stock_data.columns[:5]:  # Plot top 5 stocks
            plt.plot(stock_data[symbol].pct_change().cumsum(), label=symbol)
        plt.title('Cumulative Returns of Top 5 Stocks')
        plt.legend()

        # 2. Volatility Comparison
        plt.subplot(2, 2, 2)
        for symbol in stock_data.columns[:5]:
            volatility = tech_indicators[f'{symbol}_volatility']
            plt.plot(volatility, label=symbol)
        plt.title('Stock Volatility Comparison')
        plt.legend()

        # 3. Moving Average Analysis
        plt.subplot(2, 2, 3)
        plt.plot(stock_data['RELIANCE.NS'], label='Price')
        plt.plot(tech_indicators['RELIANCE.NS_SMA20'], label='20-day SMA')
        plt.plot(tech_indicators['RELIANCE.NS_SMA50'], label='50-day SMA')
        plt.title('Reliance Price with Moving Averages')
        plt.legend()

        # 4. Correlation Heatmap
        plt.subplot(2, 2, 4)
        sns.heatmap(stock_data.corr(),
                   annot=True,
                   cmap='RdYlBu',
                   center=0)
        plt.title('Stock Correlation Matrix')

        plt.tight_layout()
        plt.show()

        # Print performance metrics
        print("\nPerformance Metrics:")
        returns = stock_data.pct_change()
        print(f"Average Daily Returns: {returns.mean().mean():.4%}")
        print(f"Portfolio Volatility: {returns.std().mean() * np.sqrt(252):.4%}")
        print(f"Sharpe Ratio: {(returns.mean().mean() * 252) / (returns.std().mean() * np.sqrt(252)):.2f}")

    except Exception as e:
        print(f"Error in visualization: {str(e)}")

# Execute the visualization
visualize_model_performance()

"""

**Returns**
- The portfolio achieves an average daily return of 0.0757%, which translates to approximately 19.08% annually (0.0757% × 252 trading days)
- This indicates positive performance in the Indian market context

**Risk Measures**
- Portfolio volatility stands at 29.33% annually, which is relatively high
- The volatility suggests significant price fluctuations in the selected Indian stocks

**Risk-Adjusted Performance**
- Sharpe Ratio of 0.65 indicates moderate risk-adjusted returns
- This suggests the portfolio generates returns above the risk-free rate, but with considerable volatility

## Interpretation

The performance metrics reveal a portfolio that:
- Generates positive returns but with high volatility
- Shows moderate efficiency in risk-adjusted terms
- May benefit from additional risk management measures

"""

import numpy as np
import pandas as pd

# 1. Position Limits Implementation
def apply_position_limits(weights, max_position=0.3, min_position=0.05):
    """Apply position limits to portfolio weights"""
    weights = np.clip(weights, min_position, max_position)
    weights = weights / np.sum(weights)
    return weights

# 2. Defensive Stocks Addition
def add_defensive_stocks(portfolio, defensive_stocks, allocation):
    """Add defensive stocks to portfolio"""
    remaining_allocation = 1 - allocation
    scaled_portfolio = {stock: weight * remaining_allocation
                       for stock, weight in portfolio.items()}

    for stock, weight in defensive_stocks.items():
        scaled_portfolio[stock] = weight * allocation

    return scaled_portfolio

# 3. Stop Loss Implementation
def apply_stop_loss(prices, stop_loss_threshold=0.1):
    """Implement stop-loss mechanism"""
    rolling_max = prices.cummax()
    drawdown = (prices - rolling_max) / rolling_max
    stop_loss_triggered = drawdown <= -stop_loss_threshold
    return stop_loss_triggered

# 4. Sector-based Allocation
def allocate_by_sector(portfolio, sector_allocations):
    """Allocate portfolio weights by sector"""
    sector_totals = {sector: sum(portfolio[stock]
                    for stock in data['stocks'])
                    for sector, data in sector_allocations.items()}

    updated_portfolio = {}
    for sector, data in sector_allocations.items():
        for stock in data['stocks']:
            updated_portfolio[stock] = portfolio[stock] * (
                data['allocation'] / sector_totals[sector]
            )

    return updated_portfolio

# Example Usage
if __name__ == "__main__":
    # Initial portfolio
    current_portfolio = {
        'RELIANCE.NS': 0.3,
        'TCS.NS': 0.2,
        'HDFCBANK.NS': 0.2,
        'ITC.NS': 0.15,
        'HINDUNILVR.NS': 0.15
    }

    # Define defensive stocks
    defensive_stocks = {
        'HINDUNILVR.NS': 0.5,
        'ITC.NS': 0.5
    }

    # Define sector allocations
    sector_allocations = {
        'Energy': {'stocks': ['RELIANCE.NS'], 'allocation': 0.25},
        'Technology': {'stocks': ['TCS.NS'], 'allocation': 0.25},
        'Financials': {'stocks': ['HDFCBANK.NS'], 'allocation': 0.25},
        'Consumer Goods': {
            'stocks': ['ITC.NS', 'HINDUNILVR.NS'],
            'allocation': 0.25
        }
    }

    # Apply improvements
    # 1. Apply position limits
    weights = np.array(list(current_portfolio.values()))
    adjusted_weights = apply_position_limits(weights)

    # 2. Add defensive stocks
    updated_portfolio = add_defensive_stocks(
        current_portfolio,
        defensive_stocks,
        allocation=0.2
    )

    # 3. Apply sector allocation
    final_portfolio = allocate_by_sector(
        updated_portfolio,
        sector_allocations
    )

    print("Final Portfolio Allocation:")
    for stock, weight in final_portfolio.items():
        print(f"{stock}: {weight:.2%}")

def implement_risk_controls(weights, portfolio_returns):
    # Position limits
    max_position = 0.30  # 30% max per stock
    min_position = 0.05  # 5% minimum position

    # Stop-loss implementation
    stop_loss = -0.10  # 10% stop loss
    trailing_stop = -0.05  # 5% trailing stop

    # Apply position limits
    weights = np.clip(weights, min_position, max_position)
    weights = weights / np.sum(weights)  # Renormalize

    return weights

def calculate_risk_metrics(portfolio_returns):
    # Calculate key risk metrics
    annual_return = portfolio_returns.mean() * 252
    annual_vol = portfolio_returns.std() * np.sqrt(252)
    sharpe_ratio = annual_return / annual_vol

    # Maximum drawdown
    cum_returns = (1 + portfolio_returns).cumprod()
    rolling_max = cum_returns.expanding().max()
    drawdowns = cum_returns/rolling_max - 1
    max_drawdown = drawdowns.min()

    return {
        'annual_return': annual_return,
        'annual_volatility': annual_vol,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown
    }

def create_performance_dashboard():
    try:
        # Fetch and prepare data
        stock_data, nifty_data = fetch_stock_data()
        tech_indicators = calculate_technical_indicators(stock_data)
        returns = stock_data.pct_change()

        # Calculate metrics
        annual_return = returns.mean() * 252
        annual_vol = returns.std() * np.sqrt(252)
        sharpe_ratio = annual_return / annual_vol

        # 1. Cumulative Returns Plot (Separate Figure)
        plt.figure(figsize=(12, 6))
        # Select top 3 stocks for clarity
        top_stocks = ['BHARTIARTL.NS', 'HDFCBANK.NS', 'HINDUNILVR.NS']
        (1 + returns[top_stocks]).cumprod().plot()
        plt.title('Cumulative Returns', fontsize=12, pad=20)
        plt.xlabel('Date')
        plt.ylabel('Returns')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # 2. Risk Metrics (Separate Figure)
        plt.figure(figsize=(8, 5))
        metrics = pd.Series({
            'Annual\nReturn': annual_return.mean(),
            'Annual\nVolatility': annual_vol.mean(),
            'Sharpe\nRatio': sharpe_ratio.mean()
        })
        metrics.plot(kind='bar', color='skyblue')
        plt.title('Risk Metrics', fontsize=12, pad=20)
        plt.xticks(rotation=0)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # 3. Correlation Matrix (Separate Figure)
        plt.figure(figsize=(10, 8))
        sns.heatmap(returns.corr(),
                   annot=True,
                   cmap='RdYlBu',
                   center=0,
                   fmt='.2f',
                   square=True)
        plt.title('Stock Correlations', fontsize=12, pad=20)
        plt.xticks(rotation=45)
        plt.yticks(rotation=45)
        plt.tight_layout()
        plt.show()

        # Print metrics
        print("\nPortfolio Performance Metrics:")
        print(f"Annual Return: {annual_return.mean():.2%}")
        print(f"Annual Volatility: {annual_vol.mean():.2%}")
        print(f"Sharpe Ratio: {sharpe_ratio.mean():.2f}")

    except Exception as e:
        print(f"Error in dashboard creation: {str(e)}")

# Execute dashboard creation
if __name__ == "__main__":
    create_performance_dashboard()

"""Based on the portfolio performance metrics and visualizations shown:

## Performance Analysis

**Returns and Risk**
- The portfolio shows a strong annual return of 19.11%, indicating good performance in the Indian market
- However, the high annual volatility of 29.33% suggests significant price fluctuations
- The Sharpe Ratio of 0.65 indicates moderate risk-adjusted returns

## Stock Performance
- Stock A (blue line) shows the strongest performance, with cumulative returns reaching around 60%
- Stock B (orange line) shows moderate performance with higher volatility
- Stock C (green line) shows the weakest performance with declining returns

## Correlation Analysis
- The correlation matrix shows moderate to strong positive correlations between most stocks
- BHARTIARTL.NS and HINDUNILVR.NS show relatively lower correlation, suggesting diversification benefits
- Most correlations range between 0.3 to 0.4, indicating some portfolio diversification


"""

import time

def fetch_stock_data(symbols, delay=2):
    data = {}
    for symbol in symbols:
        try:
            data[symbol] = yf.download(symbol)
            time.sleep(delay)  # Add delay between requests
        except Exception as e:
            print(f"Error fetching {symbol}: {e}")
    return data

# Split symbols into smaller batches
def fetch_in_batches(symbols, batch_size=3):
    all_data = pd.DataFrame()
    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i+batch_size]
        data = yf.download(batch)
        all_data = pd.concat([all_data, data])
        time.sleep(5)  # Add delay between batches
    return all_data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def fetch_stock_data():
    dates = pd.date_range(start='2024-01-01', end='2025-02-10', freq='B')
    stocks = ['RELIANCE', 'TCS', 'HDFC', 'INFY', 'ITC']

    np.random.seed(42)
    # Generate prices instead of returns
    initial_prices = np.array([2500, 3500, 1500, 1800, 400])  # Starting prices
    daily_returns = np.random.normal(0.0001, 0.015, (len(dates), len(stocks)))

    # Convert returns to price series
    prices = np.zeros((len(dates), len(stocks)))
    prices[0] = initial_prices
    for i in range(1, len(dates)):
        prices[i] = prices[i-1] * (1 + daily_returns[i])

    stock_data = pd.DataFrame(prices, index=dates, columns=stocks)

    # Generate Nifty data
    nifty_initial = 21000
    nifty_returns = np.random.normal(0.0001, 0.012, len(dates))
    nifty_prices = nifty_initial * np.cumprod(1 + nifty_returns)
    nifty_data = pd.DataFrame(nifty_prices, index=dates, columns=['NIFTY50'])

    return stock_data, nifty_data

def create_performance_dashboard():
    try:
        plt.style.use('default')

        # Fetch data and calculate returns
        stock_data, nifty_data = fetch_stock_data()
        returns = stock_data.pct_change().dropna()

        # Calculate metrics with risk-free rate
        rf_rate = 0.05  # 5% risk-free rate
        excess_returns = returns.mean() - (rf_rate/252)  # Daily excess return

        # Annualized metrics
        annual_return = (1 + returns.mean())**252 - 1
        annual_vol = returns.std() * np.sqrt(252)
        sharpe_ratio = (excess_returns * 252) / annual_vol

        # Create visualizations (rest of the code remains the same)
        fig = plt.figure(figsize=(15, 12))
        gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.2])

        # 1. Cumulative Returns Plot
        ax1 = fig.add_subplot(gs[0, 0])
        cum_returns = (1 + returns).cumprod()
        cum_returns.plot(ax=ax1, linewidth=2)
        ax1.set_title('Cumulative Returns', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Cumulative Returns')
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # 2. Risk Metrics Bar Plot
        ax2 = fig.add_subplot(gs[0, 1])
        metrics = pd.Series({
            'Annual Return': annual_return.mean(),
            'Annual Vol': annual_vol.mean(),
            'Sharpe Ratio': sharpe_ratio.mean()
        })
        colors = ['#2ecc71', '#e74c3c', '#3498db']
        metrics.plot(kind='bar', color=colors, ax=ax2)
        ax2.set_title('Risk Metrics', fontsize=12, fontweight='bold')
        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)
        ax2.grid(True, alpha=0.3, axis='y')

        # 3. Correlation Heatmap
        ax3 = fig.add_subplot(gs[1, :])
        sns.heatmap(returns.corr(),
                   annot=True,
                   fmt='.2f',
                   cmap='RdYlBu',
                   center=0,
                   ax=ax3,
                   cbar_kws={'label': 'Correlation'})
        ax3.set_title('Stock Correlations', fontsize=12, fontweight='bold', pad=20)

        plt.tight_layout()

        # Print metrics
        print("\nPortfolio Performance Metrics:")
        print(f"Annual Return: {annual_return.mean():.2%}")
        print(f"Annual Volatility: {annual_vol.mean():.2%}")
        print(f"Sharpe Ratio: {sharpe_ratio.mean():.2f}")

        plt.show()

    except Exception as e:
        print(f"Error in dashboard creation: {str(e)}")

# Run the dashboard
create_performance_dashboard()

"""# DASHBOARD"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize

def fetch_stock_data():
    dates = pd.date_range(start='2024-01-01', end='2025-02-10', freq='B')
    stocks = ['RELIANCE', 'TCS', 'HDFC', 'INFY', 'ITC']

    np.random.seed(42)
    # Generate prices instead of returns
    initial_prices = np.array([2500, 3500, 1500, 1800, 400])
    daily_returns = np.random.normal(0.0001, 0.015, (len(dates), len(stocks)))

    prices = np.zeros((len(dates), len(stocks)))
    prices[0] = initial_prices
    for i in range(1, len(dates)):
        prices[i] = prices[i-1] * (1 + daily_returns[i])

    stock_data = pd.DataFrame(prices, index=dates, columns=stocks)

    nifty_initial = 21000
    nifty_returns = np.random.normal(0.0001, 0.012, len(dates))
    nifty_prices = nifty_initial * np.cumprod(1 + nifty_returns)
    nifty_data = pd.DataFrame(nifty_prices, index=dates, columns=['NIFTY50'])

    return stock_data, nifty_data

def optimize_portfolio(returns):
    num_assets = len(returns.columns)
    initial_weights = np.array([1/num_assets] * num_assets)

    # Portfolio optimization constraints
    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    bounds = tuple((0, 1) for _ in range(num_assets))

    # Minimize volatility
    def portfolio_volatility(weights):
        return np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))

    min_vol_result = minimize(portfolio_volatility, initial_weights,
                            method='SLSQP', bounds=bounds,
                            constraints=constraints)

    return min_vol_result.x

def generate_portfolio_analysis(returns, current_weights, optimal_weights):
    rf_rate = 0.05  # 5% risk-free rate

    # Current portfolio metrics
    current_return = np.sum(returns.mean() * current_weights) * 252
    current_vol = np.sqrt(np.dot(current_weights.T, np.dot(returns.cov() * 252, current_weights)))
    current_sharpe = (current_return - rf_rate) / current_vol

    # Optimal portfolio metrics
    opt_return = np.sum(returns.mean() * optimal_weights) * 252
    opt_vol = np.sqrt(np.dot(optimal_weights.T, np.dot(returns.cov() * 252, optimal_weights)))
    opt_sharpe = (opt_return - rf_rate) / opt_vol

    analysis = "\n=== PORTFOLIO ANALYSIS REPORT ===\n\n"

    # Current Portfolio
    analysis += "## Current Portfolio Metrics\n"
    analysis += f"Return: {current_return:.2%}\n"
    analysis += f"Risk (Volatility): {current_vol:.2%}\n"
    analysis += f"Sharpe Ratio: {current_sharpe:.2f}\n\n"

    # Optimal Portfolio
    analysis += "## Optimal Portfolio Metrics\n"
    analysis += f"Return: {opt_return:.2%}\n"
    analysis += f"Risk (Volatility): {opt_vol:.2%}\n"
    analysis += f"Sharpe Ratio: {opt_sharpe:.2f}\n\n"

    # Risk Assessment
    analysis += "## Risk Assessment\n"
    corr_matrix = returns.corr()
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i,j] > 0.7:
                high_corr_pairs.append(f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]}")

    if high_corr_pairs:
        analysis += "High Correlation Risk:\n"
        for pair in high_corr_pairs:
            analysis += f"- {pair} show strong correlation\n"

    # Performance Analysis
    analysis += "\n## Performance Analysis\n"
    stock_returns = returns.mean() * 252
    underperformers = returns.columns[stock_returns < stock_returns.mean()]
    outperformers = returns.columns[stock_returns > stock_returns.mean()]

    if len(underperformers) > 0:
        analysis += "Underperforming Stocks:\n"
        for stock in underperformers:
            analysis += f"- {stock}: {stock_returns[stock]:.2%} annual return\n"

    if len(outperformers) > 0:
        analysis += "\nOutperforming Stocks:\n"
        for stock in outperformers:
            analysis += f"- {stock}: {stock_returns[stock]:.2%} annual return\n"

    # Actionable Recommendations
    analysis += "\n## ACTIONABLE RECOMMENDATIONS\n"

    # 1. Portfolio Rebalancing
    analysis += "\n1. Portfolio Rebalancing Recommendations:\n"
    for i, (stock, curr_w, opt_w) in enumerate(zip(returns.columns, current_weights, optimal_weights)):
        diff = opt_w - curr_w
        if abs(diff) > 0.02:  # 2% threshold for recommendation
            action = "INCREASE" if diff > 0 else "DECREASE"
            analysis += f"   → {action} {stock} allocation by {abs(diff):.1%} (from {curr_w:.1%} to {opt_w:.1%})\n"

    # 2. Risk Management
    analysis += "\n2. Risk Management Recommendations:\n"
    if current_vol > opt_vol:
        reduction = (current_vol - opt_vol)/current_vol
        analysis += f"   → Implement suggested portfolio weights to reduce volatility by {reduction:.1%}\n"

    # 3. Performance Enhancement
    analysis += "\n3. Performance Enhancement Strategies:\n"
    if opt_sharpe > current_sharpe:
        improvement = (opt_sharpe - current_sharpe)/current_sharpe
        analysis += f"   → Optimize portfolio to improve Sharpe ratio by {improvement:.1%}\n"

    # 4. Diversification Advice
    max_weight = max(current_weights)
    if max_weight > 0.3:
        analysis += "\n4. Diversification Recommendations:\n"
        analysis += f"   → Reduce concentration risk - no single stock should exceed 30% of portfolio\n"

    return analysis

def create_performance_dashboard():
    try:
        plt.style.use('default')

        # Fetch data and calculate returns
        stock_data, nifty_data = fetch_stock_data()
        returns = stock_data.pct_change().dropna()

        # Current equal weights
        current_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])

        # Calculate optimal weights
        optimal_weights = optimize_portfolio(returns)

        # Calculate metrics
        annual_return = (1 + returns.mean())**252 - 1
        annual_vol = returns.std() * np.sqrt(252)
        sharpe_ratio = (annual_return - 0.05) / annual_vol  # Assuming 5% risk-free rate

        # Create visualizations
        fig = plt.figure(figsize=(15, 12))
        gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.2])

        # 1. Cumulative Returns Plot
        ax1 = fig.add_subplot(gs[0, 0])
        cum_returns = (1 + returns).cumprod()
        cum_returns.plot(ax=ax1, linewidth=2)
        ax1.set_title('Cumulative Returns', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Cumulative Returns')
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # 2. Portfolio Weights Comparison
        ax2 = fig.add_subplot(gs[0, 1])
        weights_df = pd.DataFrame({
            'Current': current_weights,
            'Optimal': optimal_weights
        }, index=returns.columns)
        weights_df.plot(kind='bar', ax=ax2)
        ax2.set_title('Portfolio Weights Comparison', fontsize=12, fontweight='bold')
        ax2.set_xlabel('Stocks')
        ax2.set_ylabel('Weight')
        ax2.legend(loc='upper right')
        ax2.grid(True, alpha=0.3)

        # 3. Correlation Heatmap
        ax3 = fig.add_subplot(gs[1, :])
        sns.heatmap(returns.corr(),
                   annot=True,
                   fmt='.2f',
                   cmap='RdYlBu',
                   center=0,
                   ax=ax3,
                   cbar_kws={'label': 'Correlation'})
        ax3.set_title('Stock Correlations', fontsize=12, fontweight='bold', pad=20)

        plt.tight_layout()

        # Generate and print analysis report
        analysis_report = generate_portfolio_analysis(returns, current_weights, optimal_weights)
        print(analysis_report)

        plt.show()

    except Exception as e:
        print(f"Error in dashboard creation: {str(e)}")

# Run the dashboard
create_performance_dashboard()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load and prepare data
iris = load_iris()
X = iris.data
y = iris.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the model
model = LogisticRegression(multi_class='ovr')
model.fit(X_train, y_train)

# Get probability predictions
y_pred_proba = model.predict_proba(X_test)

def plot_multiclass_roc(y_test, y_pred_proba, n_classes=3):
    # Binarize the labels
    y_test_bin = label_binarize(y_test, classes=range(n_classes))

    # Calculate ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    # Compute ROC curve and ROC area for each class
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot ROC curves
    plt.figure(figsize=(10, 8))

    # Plot micro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"],
             label=f'micro-average ROC curve (area = {roc_auc["micro"]:.2f})',
             color='blue', linewidth=1)

    colors = ['orange', 'green', 'red']
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i],
                label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})',
                color=colors[i], linewidth=1)

    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Extension of Receiver Operating Characteristic to multi-class')
    plt.legend(loc="lower right")
    plt.show()

# Generate the ROC curve
plot_multiclass_roc(y_test, y_pred_proba)
